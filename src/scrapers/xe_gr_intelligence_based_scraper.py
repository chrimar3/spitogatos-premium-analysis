#!/usr/bin/env python3
"""
XE.GR INTELLIGENCE-BASED SCRAPER
Using deep analysis of reconnaissance findings to target specific property patterns
"""

import asyncio
import json
import logging
import re
import csv
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import async_playwright
from urllib.parse import urljoin, urlparse

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class XEIntelligenceScraper:
    """Intelligence-based scraper using pattern analysis from reconnaissance"""
    
    def __init__(self):
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.extracted_properties = []
        
        # Intelligence: Working URL patterns discovered from reconnaissance
        self.property_url_patterns = [
            # From reconnaissance: found these working patterns
            "https://www.xe.gr/property/d/enoikiaseis-katoikion/{id}/athens-{neighborhood}",
            "https://www.xe.gr/property/d/poliseis-katoikion/{id}/athens-{neighborhood}",
            "https://xe.gr/property/d/enoikiaseis-katoikion/{id}/athens-{neighborhood}",
            "https://xe.gr/property/d/poliseis-katoikion/{id}/athens-{neighborhood}",
            "https://www.xe.gr/property/d/enoikiaseis-diamerismaton/{id}/athens-{neighborhood}",
            "https://www.xe.gr/property/d/poliseis-diamerismaton/{id}/athens-{neighborhood}"
        ]
        
        # Athens neighborhoods (Greek + English)
        self.neighborhoods = {
            "ÎšÎ¿Î»Ï‰Î½Î¬ÎºÎ¹": ["kolonaki", "ÎºÎ¿Î»Ï‰Î½Î¬ÎºÎ¹"],
            "Î Î±Î³ÎºÏÎ¬Ï„Î¹": ["pangrati", "Ï€Î±Î³ÎºÏÎ¬Ï„Î¹"],
            "Î•Î¾Î¬ÏÏ‡ÎµÎ¹Î±": ["exarchia", "ÎµÎ¾Î¬ÏÏ‡ÎµÎ¹Î±"],
            "Î Î»Î¬ÎºÎ±": ["plaka", "Ï€Î»Î¬ÎºÎ±"],
            "Î¨Ï…ÏÏÎ®": ["psirri", "ÏˆÏ…ÏÏÎ®"],
            "ÎšÏ…ÏˆÎ­Î»Î·": ["kypseli", "ÎºÏ…ÏˆÎ­Î»Î·"],
            "Î‘Î¼Ï€ÎµÎ»ÏŒÎºÎ·Ï€Î¿Î¹": ["ambelokipoi", "Î±Î¼Ï€ÎµÎ»ÏŒÎºÎ·Ï€Î¿Î¹"],
            "Î“ÎºÎ¬Î¶Î¹": ["gazi", "Î³ÎºÎ¬Î¶Î¹"],
            "ÎÎ­Î¿Ï‚ ÎšÏŒÏƒÎ¼Î¿Ï‚": ["neos-kosmos", "Î½Î­Î¿Ï‚-ÎºÏŒÏƒÎ¼Î¿Ï‚"],
            "Î ÎµÏ„ÏÎ¬Î»Ï‰Î½Î±": ["petralona", "Ï€ÎµÏ„ÏÎ¬Î»Ï‰Î½Î±"]
        }
        
        # Intelligence: ID ranges to test (discovered from reconnaissance)
        self.id_ranges = [
            (870000, 880000),  # Active ID range
            (860000, 870000),  # Secondary range
            (880000, 890000)   # Extended range
        ]
    
    async def run_intelligence_mission(self):
        """Execute intelligence-based property extraction"""
        logger.info("ğŸ§  XE.GR INTELLIGENCE-BASED EXTRACTION")
        logger.info("ğŸ¯ Using reconnaissance patterns to find real properties")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=False,
                args=['--no-sandbox', '--disable-blink-features=AutomationControlled']
            )
            
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                locale='el-GR'
            )
            
            try:
                page = await context.new_page()
                
                # Phase 1: Pattern-based URL generation and testing
                logger.info("ğŸ”® PHASE 1: Pattern-Based URL Discovery")
                await self.discover_properties_by_patterns(page)
                
                # Phase 2: Analyze successful properties for more patterns
                logger.info("ğŸ” PHASE 2: Pattern Analysis and Expansion")
                await self.analyze_and_expand_patterns(page)
                
                # Phase 3: Extract detailed data
                logger.info("ğŸ“Š PHASE 3: Property Data Extraction")
                await self.extract_verified_properties(page)
                
                # Phase 4: Export results
                logger.info("ğŸ’¾ PHASE 4: Intelligence Report Export")
                await self.export_intelligence_results()
                
            except Exception as e:
                logger.error(f"âŒ Intelligence mission failed: {e}")
            finally:
                logger.info("ğŸ” Keeping browser open for final inspection...")
                await asyncio.sleep(30)
                await browser.close()
    
    async def discover_properties_by_patterns(self, page):
        """Discover properties using URL pattern intelligence"""
        try:
            logger.info("ğŸ”® Testing URL patterns with intelligence data...")
            
            found_properties = 0
            target_properties = 20  # Target: find 20 real properties
            
            for neighborhood_greek, neighborhood_variants in self.neighborhoods.items():
                if found_properties >= target_properties:
                    break
                
                logger.info(f"ğŸ˜ï¸ Testing neighborhood: {neighborhood_greek}")
                
                for neighborhood_en in neighborhood_variants:
                    if found_properties >= target_properties:
                        break
                    
                    for start_id, end_id in self.id_ranges:
                        if found_properties >= target_properties:
                            break
                        
                        logger.info(f"ğŸ” Testing ID range {start_id}-{end_id} for {neighborhood_en}")
                        
                        # Test sample IDs from range (every 100th ID to be efficient)
                        for property_id in range(start_id, end_id, 100):
                            if found_properties >= target_properties:
                                break
                            
                            # Try all URL patterns for this ID and neighborhood
                            for pattern in self.property_url_patterns:
                                url = pattern.format(id=property_id, neighborhood=neighborhood_en)
                                
                                if await self.test_and_validate_property_url(page, url, neighborhood_greek):
                                    found_properties += 1
                                    logger.info(f"âœ… FOUND PROPERTY {found_properties}: {url}")
                                
                                await asyncio.sleep(1)  # Respectful delay
                                
                                if found_properties >= target_properties:
                                    break
            
            logger.info(f"ğŸ¯ Pattern discovery complete: {found_properties} properties found")
        
        except Exception as e:
            logger.error(f"âŒ Pattern discovery failed: {e}")
    
    async def test_and_validate_property_url(self, page, url: str, neighborhood: str) -> bool:
        """Test URL and validate it contains real property data"""
        try:
            response = await page.goto(url, wait_until="load", timeout=15000)
            
            if response and response.status == 200:
                await asyncio.sleep(2)
                
                # Get page content for validation
                content = await page.content()
                page_text = await page.inner_text('body')
                
                # Enhanced validation: must contain real property indicators
                property_indicators = [
                    'Ï„Î¹Î¼Î®', 'price', 'Ï„.Î¼', 'mÂ²', 'ÎµÎ½Î¿Î¹ÎºÎ¯Î±ÏƒÎ·', 'Ï€ÏÎ»Î·ÏƒÎ·',
                    'Î´Î¹Î±Î¼Î­ÏÎ¹ÏƒÎ¼Î±', 'apartment', 'ÎµÎ½ÎµÏÎ³ÎµÎ¹Î±ÎºÎ®', 'Î´Ï‰Î¼Î¬Ï„Î¹Î±'
                ]
                
                indicator_count = sum(1 for indicator in property_indicators 
                                    if indicator.lower() in content.lower())
                
                # Location validation: must mention Athens or the specific neighborhood
                location_indicators = [
                    'Î±Î¸Î®Î½Î±', 'athens', neighborhood.lower(),
                    'attiki', 'Î±Ï„Ï„Î¹ÎºÎ®', 'ÎµÎ»Î»Î¬Î´Î±', 'greece'
                ]
                
                has_location = any(loc.lower() in content.lower() for loc in location_indicators)
                
                # Content validation: must be substantial property page
                is_substantial = len(page_text) > 3000  # Real property pages are detailed
                
                # Price validation: must contain realistic price patterns
                price_patterns = [
                    r'â‚¬\s*\d{2,6}',  # Euro prices
                    r'\d{2,6}\s*â‚¬',  # Prices with euro
                    r'\d{1,3}(?:\.\d{3})*\s*â‚¬'  # Formatted prices
                ]
                
                has_price = any(re.search(pattern, content) for pattern in price_patterns)
                
                # Area validation: must mention square meters
                area_patterns = [
                    r'\d+\s*Ï„\.?Î¼\.?',
                    r'\d+\s*mÂ²',
                    r'\d+\s*sqm'
                ]
                
                has_area = any(re.search(pattern, content, re.IGNORECASE) for pattern in area_patterns)
                
                # Property is valid if it meets multiple criteria
                validation_score = sum([
                    indicator_count >= 4,  # Has property terms
                    has_location,          # Has location info
                    is_substantial,        # Substantial content
                    has_price,            # Has pricing
                    has_area              # Has area info
                ])
                
                if validation_score >= 3:  # Must meet at least 3/5 criteria
                    logger.info(f"âœ… VALIDATED PROPERTY: {url} (score: {validation_score}/5)")
                    
                    self.extracted_properties.append({
                        'url': url,
                        'neighborhood': neighborhood,
                        'validation_score': validation_score,
                        'indicator_count': indicator_count,
                        'has_location': has_location,
                        'has_price': has_price,
                        'has_area': has_area,
                        'content_length': len(page_text),
                        'discovery_method': 'intelligence_pattern',
                        'discovered_at': datetime.now().isoformat()
                    })
                    
                    return True
                else:
                    logger.debug(f"âŒ Invalid property: {url} (score: {validation_score}/5)")
            
            return False
            
        except Exception as e:
            logger.debug(f"âŒ URL test failed {url}: {e}")
            return False
    
    async def analyze_and_expand_patterns(self, page):
        """Analyze successful URLs to discover new patterns"""
        try:
            logger.info("ğŸ” Analyzing successful URLs for pattern expansion...")
            
            if not self.extracted_properties:
                logger.warning("âš ï¸ No properties found for pattern analysis")
                return
            
            # Analyze successful URL patterns
            successful_urls = [prop['url'] for prop in self.extracted_properties]
            
            # Extract ID patterns from successful URLs
            successful_ids = []
            for url in successful_urls:
                id_match = re.search(r'/(\d{6,8})/', url)
                if id_match:
                    successful_ids.append(int(id_match.group(1)))
            
            if successful_ids:
                # Find ID clusters and test nearby IDs
                logger.info(f"ğŸ¯ Found {len(successful_ids)} successful IDs, testing nearby ranges...")
                
                target_additional = 10
                found_additional = 0
                
                for base_id in successful_ids[:3]:  # Test around first 3 successful IDs
                    if found_additional >= target_additional:
                        break
                    
                    # Test IDs around successful ones (Â±20 range)
                    for offset in range(-20, 21, 5):
                        if found_additional >= target_additional:
                            break
                        
                        test_id = base_id + offset
                        if test_id <= 0:
                            continue
                        
                        # Test this ID with all neighborhoods and patterns
                        for neighborhood_greek, variants in list(self.neighborhoods.items())[:3]:
                            if found_additional >= target_additional:
                                break
                            
                            for variant in variants[:1]:  # Test primary variant
                                for pattern in self.property_url_patterns[:2]:  # Test main patterns
                                    test_url = pattern.format(id=test_id, neighborhood=variant)
                                    
                                    # Skip if we already tested this URL
                                    if test_url in [prop['url'] for prop in self.extracted_properties]:
                                        continue
                                    
                                    if await self.test_and_validate_property_url(page, test_url, neighborhood_greek):
                                        found_additional += 1
                                        logger.info(f"âœ… PATTERN EXPANSION: Found property {found_additional}")
                                    
                                    await asyncio.sleep(1)
                
                logger.info(f"ğŸ” Pattern expansion found {found_additional} additional properties")
        
        except Exception as e:
            logger.error(f"âŒ Pattern analysis failed: {e}")
    
    async def extract_verified_properties(self, page):
        """Extract detailed data from all verified properties"""
        try:
            logger.info(f"ğŸ“Š Extracting detailed data from {len(self.extracted_properties)} verified properties...")
            
            for i, property_info in enumerate(self.extracted_properties):
                url = property_info['url']
                logger.info(f"ğŸ“‹ Extracting {i+1}/{len(self.extracted_properties)}: {url}")
                
                try:
                    response = await page.goto(url, wait_until="networkidle", timeout=20000)
                    
                    if response and response.status == 200:
                        await asyncio.sleep(3)
                        
                        # Extract comprehensive property data
                        property_data = await self.extract_comprehensive_data(page, url, property_info)
                        
                        if property_data:
                            # Update the property info with extracted data
                            property_info.update(property_data)
                            logger.info(f"âœ… Extracted: {property_data.get('title', 'No title')[:50]}...")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to extract {url}: {e}")
                    continue
                
                await asyncio.sleep(2)  # Respectful delay
        
        except Exception as e:
            logger.error(f"âŒ Property data extraction failed: {e}")
    
    async def extract_comprehensive_data(self, page, url: str, existing_info: Dict) -> Optional[Dict]:
        """Extract comprehensive property data"""
        try:
            page_text = await page.inner_text('body')
            title = await page.title()
            
            property_data = {
                'title': title,
                'raw_text_length': len(page_text),
                'extraction_timestamp': datetime.now().isoformat()
            }
            
            # Enhanced price extraction
            price_patterns = [
                r'(\d{1,3}(?:\.\d{3})*)\s*â‚¬',
                r'â‚¬\s*(\d{1,3}(?:\.\d{3})*)',
                r'Ï„Î¹Î¼Î®[:\s]*(\d{1,3}(?:\.\d{3})*)',
                r'Î¤Î¹Î¼Î®[:\s]*(\d{1,3}(?:\.\d{3})*)',
                r'Î¤Î™ÎœÎ—[:\s]*(\d{1,3}(?:\.\d{3})*)'
            ]
            
            for pattern in price_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    try:
                        price_str = match.group(1).replace('.', '')
                        price = float(price_str)
                        if 50 <= price <= 10000000:  # Realistic price range
                            property_data['price'] = price
                            break
                    except ValueError:
                        continue
            
            # Enhanced SQM extraction
            sqm_patterns = [
                r'(\d+(?:[.,]\d+)?)\s*Ï„\.?Î¼\.?',
                r'(\d+(?:[.,]\d+)?)\s*mÂ²',
                r'(\d+(?:[.,]\d+)?)\s*sq\.?m',
                r'ÎµÎ¼Î²Î±Î´ÏŒÎ½[:\s]*(\d+(?:[.,]\d+)?)',
                r'Î•Î¼Î²Î±Î´ÏŒÎ½[:\s]*(\d+(?:[.,]\d+)?)'
            ]
            
            for pattern in sqm_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    try:
                        sqm = float(match.group(1).replace(',', '.'))
                        if 10 <= sqm <= 1000:  # Realistic area range
                            property_data['sqm'] = sqm
                            break
                    except ValueError:
                        continue
            
            # Enhanced energy class extraction
            energy_patterns = [
                r'ÎµÎ½ÎµÏÎ³ÎµÎ¹Î±ÎºÎ®\s+ÎºÎ»Î¬ÏƒÎ·[:\s]*([A-G][+\-]?)',
                r'Î•Î½ÎµÏÎ³ÎµÎ¹Î±ÎºÎ®\s+ÎºÎ»Î¬ÏƒÎ·[:\s]*([A-G][+\-]?)',
                r'Î•ÎÎ•Î¡Î“Î•Î™Î‘ÎšÎ—\s+ÎšÎ›Î‘Î£Î—[:\s]*([A-G][+\-]?)',
                r'energy\s+class[:\s]*([A-G][+\-]?)',
                r'Energy\s+Class[:\s]*([A-G][+\-]?)'
            ]
            
            for pattern in energy_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    energy_class = match.group(1).upper()
                    if energy_class in ['A+', 'A', 'B+', 'B', 'C+', 'C', 'D', 'E', 'F', 'G']:
                        property_data['energy_class'] = energy_class
                        break
            
            # Address extraction
            address_patterns = [
                r'Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·[:\s]*([^,\n\.]{10,100})',
                r'Î”Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·[:\s]*([^,\n\.]{10,100})',
                r'Ï€ÎµÏÎ¹Î¿Ï‡Î®[:\s]*([^,\n\.]{10,100})',
                r'Î ÎµÏÎ¹Î¿Ï‡Î®[:\s]*([^,\n\.]{10,100})',
                r'Î‘Î¸Î®Î½Î±[,\s]*([^,\n\.]{5,80})'
            ]
            
            for pattern in address_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    address = match.group(1).strip()
                    if len(address) > 5 and 'Î±Î¸Î®Î½Î±' in address.lower() or existing_info['neighborhood'].lower() in address.lower():
                        property_data['address'] = address
                        break
            
            # Property type extraction
            property_types = {
                'Î´Î¹Î±Î¼Î­ÏÎ¹ÏƒÎ¼Î±': 'Î”Î¹Î±Î¼Î­ÏÎ¹ÏƒÎ¼Î±',
                'Î¼Î¿Î½Î¿ÎºÎ±Ï„Î¿Î¹ÎºÎ¯Î±': 'ÎœÎ¿Î½Î¿ÎºÎ±Ï„Î¿Î¹ÎºÎ¯Î±',
                'Î¼ÎµÎ¶Î¿Î½Î­Ï„Î±': 'ÎœÎµÎ¶Î¿Î½Î­Ï„Î±',
                'ÏÎµÏ„Î¹ÏÎ­': 'Î¡ÎµÏ„Î¹ÏÎ­',
                'apartment': 'Î”Î¹Î±Î¼Î­ÏÎ¹ÏƒÎ¼Î±',
                'house': 'ÎœÎ¿Î½Î¿ÎºÎ±Ï„Î¿Î¹ÎºÎ¯Î±'
            }
            
            for type_key, type_value in property_types.items():
                if re.search(type_key, page_text, re.IGNORECASE):
                    property_data['property_type'] = type_value
                    break
            
            # Listing type
            if re.search(r'ÎµÎ½Î¿Î¹ÎºÎ¯Î±ÏƒÎ·|rental|rent|Ï€ÏÎ¿Ï‚ ÎµÎ½Î¿Î¹ÎºÎ¯Î±ÏƒÎ·', page_text, re.IGNORECASE):
                property_data['listing_type'] = 'Î•Î½Î¿Î¹ÎºÎ¯Î±ÏƒÎ·'
            elif re.search(r'Ï€ÏÎ»Î·ÏƒÎ·|sale|sell|Ï€ÏÎ¿Ï‚ Ï€ÏÎ»Î·ÏƒÎ·', page_text, re.IGNORECASE):
                property_data['listing_type'] = 'Î ÏÎ»Î·ÏƒÎ·'
            
            # Rooms extraction
            rooms_patterns = [
                r'(\d+)\s*Î´Ï‰Î¼Î¬Ï„Î¹',
                r'(\d+)\s*Ï…Ï€Î½Î¿Î´Ï‰Î¼Î¬Ï„Î¹',
                r'(\d+)\s*room',
                r'(\d+)\s*bedroom'
            ]
            
            for pattern in rooms_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    try:
                        rooms = int(match.group(1))
                        if 1 <= rooms <= 10:
                            property_data['rooms'] = rooms
                            break
                    except ValueError:
                        continue
            
            # Floor extraction
            floor_patterns = [
                r'(\d+)Î¿Ï‚\s*ÏŒÏÎ¿Ï†Î¿Ï‚',
                r'ÏŒÏÎ¿Ï†Î¿Ï‚[:\s]*(\d+)',
                r'ÎŒÏÎ¿Ï†Î¿Ï‚[:\s]*(\d+)',
                r'floor[:\s]*(\d+)'
            ]
            
            for pattern in floor_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    property_data['floor'] = f"{match.group(1)}Î¿Ï‚"
                    break
            
            # Data completeness score
            key_fields = ['price', 'sqm', 'energy_class', 'address', 'property_type']
            filled_fields = sum(1 for field in key_fields if property_data.get(field))
            property_data['data_completeness'] = filled_fields / len(key_fields)
            
            # Must have at least price OR sqm to be valid
            if property_data.get('price') or property_data.get('sqm'):
                return property_data
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ Comprehensive data extraction failed: {e}")
            return None
    
    async def export_intelligence_results(self):
        """Export intelligence-based extraction results"""
        try:
            logger.info("ğŸ’¾ Exporting intelligence results...")
            
            # Filter for properties with meaningful data
            valid_properties = [
                prop for prop in self.extracted_properties 
                if prop.get('price') or prop.get('sqm')
            ]
            
            if not valid_properties:
                logger.warning("âš ï¸ No valid properties extracted")
                return
            
            # Export to JSON
            json_file = f'outputs/xe_gr_intelligence_{self.session_id}.json'
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'intelligence_metadata': {
                        'session_id': self.session_id,
                        'extraction_timestamp': datetime.now().isoformat(),
                        'total_properties': len(valid_properties),
                        'method': 'intelligence_pattern_analysis',
                        'url_patterns_tested': len(self.property_url_patterns),
                        'neighborhoods_tested': len(self.neighborhoods)
                    },
                    'properties': valid_properties
                }, f, indent=2, ensure_ascii=False)
            
            # Export to CSV
            csv_file = f'outputs/xe_gr_intelligence_{self.session_id}.csv'
            if valid_properties:
                fieldnames = [
                    'url', 'neighborhood', 'title', 'price', 'sqm', 'energy_class',
                    'address', 'property_type', 'listing_type', 'rooms', 'floor',
                    'validation_score', 'data_completeness', 'discovery_method',
                    'extraction_timestamp'
                ]
                
                with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    for prop in valid_properties:
                        writer.writerow({key: prop.get(key, '') for key in fieldnames})
            
            # Generate intelligence report
            logger.info("\n" + "="*80)
            logger.info("ğŸ§  XE.GR INTELLIGENCE EXTRACTION - FINAL REPORT")
            logger.info("="*80)
            logger.info(f"âœ… REAL Properties Extracted: {len(valid_properties)}")
            
            # Data quality analysis
            with_price = sum(1 for p in valid_properties if p.get('price'))
            with_sqm = sum(1 for p in valid_properties if p.get('sqm'))
            with_energy = sum(1 for p in valid_properties if p.get('energy_class'))
            with_address = sum(1 for p in valid_properties if p.get('address'))
            
            logger.info(f"ğŸ’° Properties with Price: {with_price}")
            logger.info(f"ğŸ“ Properties with SQM: {with_sqm}")
            logger.info(f"âš¡ Properties with Energy Class: {with_energy}")
            logger.info(f"ğŸ“ Properties with Address: {with_address}")
            
            # Validation score analysis
            if valid_properties:
                avg_validation = sum(p.get('validation_score', 0) for p in valid_properties) / len(valid_properties)
                avg_completeness = sum(p.get('data_completeness', 0) for p in valid_properties) / len(valid_properties)
                logger.info(f"ğŸ“Š Average Validation Score: {avg_validation:.2f}/5")
                logger.info(f"ğŸ“Š Average Data Completeness: {avg_completeness:.2f}")
            
            logger.info(f"\nğŸ’¾ Results saved:")
            logger.info(f"   ğŸ“„ JSON: {json_file}")
            logger.info(f"   ğŸ“Š CSV: {csv_file}")
            
            # Show sample results
            if valid_properties:
                logger.info(f"\nğŸ  SAMPLE PROPERTIES:")
                for i, prop in enumerate(valid_properties[:5], 1):
                    title = prop.get('title', 'No title')[:50]
                    price = prop.get('price', 'N/A')
                    sqm = prop.get('sqm', 'N/A')
                    energy = prop.get('energy_class', 'N/A')
                    neighborhood = prop.get('neighborhood', 'N/A')
                    logger.info(f"   {i}. {neighborhood} | {title}... | â‚¬{price} | {sqm}mÂ² | Energy {energy}")
            
            logger.info("="*80)
            
        except Exception as e:
            logger.error(f"âŒ Intelligence results export failed: {e}")

async def main():
    """Run intelligence-based extraction"""
    scraper = XEIntelligenceScraper()
    await scraper.run_intelligence_mission()

if __name__ == "__main__":
    asyncio.run(main())